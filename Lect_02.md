# Лекция 2 (12.02.26): Линейные коды в обработке сигналов

### Задача 1: (5,2)-код, вероятность ошибки в BSC, порождающая и проверочная матрицы
Код линейный, поскольку сумма любых двух кодовых слов также является кодовым словом. Для нахождения порождающей матрицы $G$ выбираются кодовые слова, соответствующие единичным векторам информационных символов. Например, для информационного слова $(1,0)$ и $(0,1)$ формируется матрица $G$ размера $2 \times 5$.

$$
G = \begin{pmatrix}
1 & 0 & 1 & 1 & 0 \\
0 & 1 & 1 & 0 & 1
\end{pmatrix}
$$

Проверка для $(1,1)$: $(1,1,0,1,1)$, что соответствует линейной комбинации строк $G$.

Для проверочной матрицы $H$ (размера $3 \times 5$) приводим $G$ к систематическому виду путем перестановки строк/столбцов, где левая часть — единичная матрица $I_2$, правая — $P$. Тогда $H = [-P^T \mid I_3]$.

Минимальное расстояние Хэмминга $d_{\min} = 3$ (исправляет $t=1$ ошибку). Вероятность ошибки в бинарном симметричном канале (BSC) с вероятностью перехода $p$ рассчитывается аналитически или перебором. Основной вклад от ошибок веса 2 (кратность 2 дает $10 p^2 (1-p)^3$), с учетом, что некоторые ошибки веса 2 могут быть исправлены. Пример расчета: $P_e \approx 8 \times 10^{-6}$ (аналитически) или $\approx 10^{-5}$ (перебором для конкретных случаев). Учитывается, что код иногда исправляет и двойные ошибки, несмотря на $d_{\min}=3$.

### Задача 2: Расстояние Хэмминга как метрика
Доказывается, что расстояние Хэмминга $d(x,y)$ (число позиций, где $x$ и $y$ различаются) удовлетворяет аксиомам метрики:
1. **Неотрицательность**: $d(x,y) \ge 0$, с $d(x,y)=0$ iff $x=y$ (очевидно из определения).
2. **Симметричность**: $d(x,y) = d(y,x)$ (сравнение симметрично).
3. **Неравенство треугольника**: $d(x,z) \le d(x,y) + d(y,z)$ (рассматриваются все комбинации различий по координатам; сумма различий не превышает).

Аксиомы проверяются для бинарных векторов, подтверждая использование $d$ как метрики в пространстве кодов.

## 1. Основные понятия и эффективность кодов
**Код** — это набор (множество) **кодовых слов** (codewords). Информационные символы генерируются источником с равной вероятностью (все $2^k$ комбинаций для двоичного случая). Если вероятности неравны, как в коде Хаффмана, возникает энтропия, которую можно сжимать.

**Эффективные коды** должны удовлетворять двум требованиям:
- **Длинные** ($n$ большое) → большое **минимальное расстояние** $d_{\min}$ (способность исправлять ошибки). Большое $n$ повышает шансы на большее $d_{\min}$.
- Низкая **сложность** кодера и декодера (чтобы избежать экспоненциального роста).

Линейные коды снижают сложность: кодирование сводится к матричной операции.

**Линейный код** $(n, k)$ над полем $\mathrm{GF}(2)$:
- Все кодовые слова образуют **линейное подпространство** размерности $k$ в $\mathbb{F}_2^n$.
- **Порождающая матрица** $G$ размера $k \times n$ — строки являются **базисом** пространства кода.
- Кодовое слово:  
  $$
  \mathbf{c} = \mathbf{m} \cdot G, \quad \mathbf{m} = (m_1, \dots, m_k) \in \mathbb{F}_2^k
  $$
Кодовые слова — линейные комбинации строк $G$.

## 2. Проверочная матрица $H$
**Проверочная матрица** $H$ размера $(n-k) \times n$:
- Любое кодовое слово удовлетворяет  
  $$
  \mathbf{c} \cdot H^T = \mathbf{0}
  $$
- Ортогональность:  
  $$
  G \cdot H^T = \mathbf{0}
  $$

**Интерпретация**:
- Столбцы $H$ задают **линейные проверки** на кодовые слова (скалярное произведение $\mathbf{c} \cdot \mathbf{h} = 0$ для каждого $\mathbf{h}$ — строки $H^T$).
- Пространство проверок — ортогональное дополнение к коду, размерность $n-k$ (ранг $H = n-k$).

**Синдром** при получении вектора $\mathbf{r} = \mathbf{c} + \mathbf{e}$:  
$$
\mathbf{s} = \mathbf{r} \cdot H^T = \mathbf{e} \cdot H^T
$$
(если $\mathbf{s} = \mathbf{0}$ — ошибок нет). Для одиночной ошибки $\mathbf{e}$ (вес 1) синдром $\mathbf{s}$ равен столбцу $H$ на позиции ошибки.

## 3. Систематический вид кода
$$
G_{\mathrm{sys}} = [I_k \mid P], \quad H_{\mathrm{sys}} = [-P^T \mid I_{n-k}]
$$
где $I$ — единичная матрица. Информационные биты — первые $k$ символов кодового слова, остальные — проверочные.

**Пример** (код $(7,4)$ Хэмминга):
$$
G = \begin{pmatrix}
1 & 0 & 0 & 0 & 1 & 1 & 0 \\
0 & 1 & 0 & 0 & 1 & 0 & 1 \\
0 & 0 & 1 & 0 & 0 & 1 & 1 \\
0 & 0 & 0 & 1 & 1 & 1 & 1
\end{pmatrix}, \quad
H = \begin{pmatrix}
1 & 1 & 0 & 1 & 1 & 0 & 0 \\
1 & 0 & 1 & 1 & 0 & 1 & 0 \\
0 & 1 & 1 & 1 & 0 & 0 & 1
\end{pmatrix}
$$

## 4. Минимальное расстояние $d_{\min}$
Для **линейного** кода:
$$
d_{\min} = \min_{\mathbf{c} \neq \mathbf{0}} w(\mathbf{c})
$$
(минимальный вес ненулевого кодового слова).

**Теорема** (связь с $H$):
- $d_{\min} \ge d$ ⇔ **любые** $d-1$ столбцов $H$ **линейно независимы**.
- $d_{\min} = d$ ⇔ **существуют** $d$ линейно зависимых столбцов $H$ и любые меньшие — независимы. Таким образом, $d_{\min}$ — минимальное число линейно зависимых столбцов $H$.

**Граница Синглтона**: $d_{\min} \le n - k + 1$ (из ранга $H = n-k$; любые $n-k+1$ столбцов зависимы).

**Следствие** (для исправления $t$ ошибок):
$$
d_{\min} \ge 2t + 1
$$

## 5. Пример: Код с проверкой на четность
$(n, n-1)$-код, $H = [1, 1, \dots, 1]$ (строка из единиц). $G = [I_{n-1} \mid \mathbf{1}^T]$, где $\mathbf{1}$ — столбец единиц. $d_{\min} = 2$ (минимальный вес 2). Все кодовые слова имеют четный вес. Обнаруживает ошибки нечетного веса (синдром $\neq 0$), но не исправляет (не отличает от других кодовых слов). Для $r=2$ в конструкции Хэмминга — это код с четностью.

## 6. Код Хэмминга $(2^r-1, 2^r-1-r)$
- $d_{\min} = 3$ (исправляет 1 ошибку).
- Столбцы $H$ — **все ненулевые** векторы длины $r$ (в двоичном представлении, от 1 до $2^r-1$).
- Пример $(7,4)$ приведён выше.
- Оптимален: нет кода (даже нелинейного) с большим числом слов при той же $n$ и $d=3$ (удовлетворяет границе Хэмминга, совершенный код).
- Каждая строка $H$ имеет $2^{r-1}$ единиц (половина, если считать нулевой столбец).
- Декодирование: синдром указывает позицию ошибки (номер столбца $H$ в двоичном виде).

## 7. Расширенный код Хэмминга $(2^r, 2^r-r-1)$
- Добавляется нулевой столбец к $H$ Хэмминга и строка из единиц (проверка на четность).
- $d_{\min} = 4$ (исправляет 1 ошибку, обнаруживает некоторые веса 2).
- Пример: из $(7,4)$ получается $(8,4)$.
- Нулевой столбец увеличивает $n$, не меняя $d$; четность удваивает способность обнаружения.

## 8. Дуальные коды
- **Дуальный код** $C^\perp$ — ортогональное дополнение к $C$.
- Если $G$ порождает $C$, то $H$ порождает $C^\perp$ и наоборот ($G^\perp = H$, $H^\perp = G$).
- Для кода Хэмминга дуальный код — **симплексный код**: все слова веса $2^{r-1}$, $d^\perp = 2^{r-1}$ (расстояние между любыми словами одинаково).
- Дуальный к расширенному Хэммингу — **код Рида-Маллера первого порядка**.

## 9. Дополнительные замечания
- **Количество проверок** всегда $n-k$.
- **Сложность декодирования** определяется по синдрому (таблица синдромов для малых $n$). На следующей лекции — синдромное декодирование (можно написать программу для демонстрации).
- Коды Хэмминга **оптимальны** по границе Хэмминга (совершенные коды).
- Нужно соблюдать баланс между длиной, расстоянием и сложностью; линейность упрощает операции.
